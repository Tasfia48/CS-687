# -*- coding: utf-8 -*-
"""Question 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z8oyQEqLWJi9jFHLmsmpJVGi8sXmd235

Loading Libraries
"""

import numpy as np
import matplotlib.pyplot as plt

"""####Question 2"""

#Required functions
def set_prob(n,p):
  prob=[]
  for i in range(0,n):
    # print(i)
    # print(prob)
    prob.append(p[i])
  return prob  

def get_reward(i,prob): #get reward if ith arm was selected
  if np.random.random() < prob[i]:
      return 1
  else:
      return 0

def calculate_optimal_action(iters,arr,optimal):
  optimal_res = []
  for i in range(iters):
    subarr = [a[i] for a in arr]
    v = subarr.count(optimal)/len(subarr)
    optimal_res.append(v)
  return optimal_res 
  # print(i,v)

def calculate_mean_regret(iters,arr):
  optimal_res = []
  for i in range(iters):
    subarr = [a[i] for a in arr]
    v = np.mean(subarr)
    optimal_res.append(v)
  return optimal_res 
  # print(i,v)

"""### Greedy Algorithm"""

#Greedy Algorithm

def Greedy(n,prob,iteration,init_proba=1):
  counts = [0] * n
  actions = []  # A list of machine ids, 0 to bandit.n-1.
  regret = 0  # Cumulative regret.
  regrets = [0]  # History of cumulative regret.
  reward = [0] * n
  estimates = [init_proba] *n # Optimistic initialization
  step_regret=[]* n

  for iter in range(iteration):
    i = max(range(n), key=lambda x: estimates[x])
    r = get_reward(i,prob)
    reward[i]+=r
    # print(r)
    estimates[i] = 1. / (counts[i] + 1) * reward[i]
    # print(estimates)

    counts[i] += 1
    # print(actions)
    actions.append(i)
    best_prob = max(prob)
    regret += best_prob - prob[i]
    # print(regret)
    regrets.append(regret)
    step_regret.append(best_prob - prob[i])
    # regrets,regret=update_regret(i,prob,regret,regrets)
  return regrets,estimates,actions,step_regret

"""### UCB1"""

#UCB1 Algorithm

def UCB(n,prob,iteration,init_proba=1):
  t=0
  counts = [0] * n
  actions = []  # A list of machine ids, 0 to bandit.n-1.
  regret = 0  # Cumulative regret.
  regrets = [0]  # History of cumulative regret.
  reward = [0] * n
  estimates = [init_proba] *n # Optimistic initialization
  step_regret =[]

  for iter in range(iteration):
    t+=1
    i = max(range(n), key=lambda x: estimates[x] + np.sqrt(2 * np.log(t) / (1 + counts[x])))
    r = get_reward(i,prob)
    reward[i]+=r
    # print(r)
    estimates[i] = 1. / (counts[i] + 1) * reward[i]
    # print(estimates)

    counts[i] += 1
    # print(actions)
    actions.append(i)
    best_prob = max(prob)
    regret += best_prob - prob[i]
    # print(regret)
    regrets.append(regret)
    step_regret.append(best_prob - prob[i])
    # regrets,regret=update_regret(i,prob,regret,regrets)
  return regrets,estimates,actions,step_regret

"""### Thompson Sampling"""

#Thompson Sampling

def TSampling(n,prob,iteration,init_a=1,init_b=1):
  a = [init_a] * n
  b = [init_b] * n
  counts = [0] * n
  actions = []  # A list of machine ids, 0 to bandit.n-1.
  regret = 0  # Cumulative regret.
  regrets = [0.]  # History of cumulative regret.
  reward = [0] * n
  estimates = (a[i] / (a[i] + b[i]) for i in range(n) )# estimated utility based on prior belief
  step_regret=[]

  for iter in range(iteration):
    samples = [np.random.beta(a[x], b[x]) for x in range(n)]
    i = max(range(n), key=lambda x: samples[x])
    r = get_reward(i,prob)

    a[i] += r
    b[i] += (1 - r)
    reward[i]+=r
    # print(r)
    # estimates[i] = 1. / (counts[i] + 1) * reward[i]
    # print(estimates)

    counts[i] += 1
    # print(actions)
    actions.append(i)
    best_prob = max(prob)
    regret += best_prob - prob[i]
    # print(regret)
    regrets.append(regret)
    step_regret.append(best_prob - prob[i])
    # regrets,regret=update_regret(i,prob,regret,regrets)
  return regrets,estimates,actions,step_regret

"""### Nomalized Mean Regret at each step

k=11
"""

k = 11
prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
iters = 1000
eps_regret = np.zeros(iters+1)
UCB_regret = np.zeros(iters+1)
T_regret = np.zeros(iters+1)
Runs = 300
Garr = []
Tarr = []
Uarr = []
string = "k11-1000regret"
optimal_actions= np.zeros(iters+1)
optimal = np.argmax(prob)
# Run experiments
for i in range(Runs):
    # Initialize bandits
    Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
    Garr.append(Gstep_regret)
    Gmean = calculate_mean_regret(iters,Garr)

    Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
    Uarr.append(Ustep_regret)
    Umean = calculate_mean_regret(iters,Uarr)

    Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    Tarr.append(Tstep_regret)
    Tmean = calculate_mean_regret(iters,Tarr)

    
plt.figure(figsize=(8,6))
plt.plot(Gmean, label="Greedy")
plt.plot(Umean, label="UCB")
plt.plot(Tmean, label="$Thompson Sampling$")
plt.legend()
plt.xlabel("Time",fontsize=14)
plt.ylabel("Nomalized Mean Regret",fontsize=14)
plt.title("# of time-steps " + str(iters))
plt.savefig(string+".pdf")
plt.show()

# k = 11
# prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# iters = 10000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k11-10000regret"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gstep_regret)
#     Gmean = calculate_mean_regret(iters,Garr)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Ustep_regret)
#     Umean = calculate_mean_regret(iters,Uarr)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tstep_regret)
#     Tmean = calculate_mean_regret(iters,Tarr)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Nomalized Mean Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

# k = 11
# prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# iters = 100000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k11-100000regret"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gstep_regret)
#     Gmean = calculate_mean_regret(iters,Garr)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Ustep_regret)
#     Umean = calculate_mean_regret(iters,Uarr)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tstep_regret)
#     Tmean = calculate_mean_regret(iters,Tarr)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Nomalized Mean Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

"""K=5"""

k = 5
prob = [0.3, 0.5, 0.7, 0.83, 0.85]
iters = 1000
eps_regret = np.zeros(iters+1)
UCB_regret = np.zeros(iters+1)
T_regret = np.zeros(iters+1)
Runs = 300
Garr = []
Tarr = []
Uarr = []
string = "k51000regret"
optimal_actions= np.zeros(iters+1)
optimal = np.argmax(prob)
# Run experiments
for i in range(Runs):
    # Initialize bandits
    Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
    Garr.append(Gstep_regret)
    Gmean = calculate_mean_regret(iters,Garr)

    Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
    Uarr.append(Ustep_regret)
    Umean = calculate_mean_regret(iters,Uarr)

    Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    Tarr.append(Tstep_regret)
    Tmean = calculate_mean_regret(iters,Tarr)

    
plt.figure(figsize=(8,6))
plt.plot(Gmean, label="Greedy")
plt.plot(Umean, label="UCB")
plt.plot(Tmean, label="$Thompson Sampling$")
plt.legend()
plt.xlabel("Time",fontsize=14)
plt.ylabel("Nomalized Mean Regret",fontsize=14)
plt.title("# of time-steps " + str(iters))
plt.savefig(string+".pdf")
plt.show()

# k = 5
# prob = [0.3, 0.5, 0.7, 0.83, 0.85]
# iters = 10000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k510000regret"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gstep_regret)
#     Gmean = calculate_mean_regret(iters,Garr)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Ustep_regret)
#     Umean = calculate_mean_regret(iters,Uarr)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tstep_regret)
#     Tmean = calculate_mean_regret(iters,Tarr)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Nomalized Mean Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

# k = 5
# prob = [0.3, 0.5, 0.7, 0.83, 0.85]
# iters = 100000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k5100000regret"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gstep_regret)
#     Gmean = calculate_mean_regret(iters,Garr)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Ustep_regret)
#     Umean = calculate_mean_regret(iters,Uarr)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tstep_regret)
#     Tmean = calculate_mean_regret(iters,Tarr)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Nomalized Mean Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

"""### Cummilative Regret

### n= 300, iteration = 1000, k=11
"""

k = 11
prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
iters = 1000
eps_regret = np.zeros(iters+1)
UCB_regret = np.zeros(iters+1)
T_regret = np.zeros(iters+1)
Runs = 300
string = "k11-1000"

# Run experiments
for i in range(Runs):
    # Initialize bandits
    Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
    Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
    Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    
    # Update long-term averages
    eps_regret = eps_regret + (Gregrets - eps_regret) / (i + 1)
    UCB_regret = UCB_regret + (Uregrets - UCB_regret) / (i + 1)
    T_regret = T_regret + (Tregrets - T_regret) / (i + 1)        

    
plt.figure(figsize=(8,6))
plt.plot(eps_regret, label="Greedy")
plt.plot(UCB_regret, label="UCB")
plt.plot(T_regret, label="$Thompson Sampling$")
plt.legend()
plt.xlabel("Time",fontsize=14)
plt.ylabel("Average Regret",fontsize=14)
plt.title("# of time-steps " + str(iters))
plt.savefig(string+".pdf")
plt.show()

"""n= 300, iteration = 10000, k=11"""

# k = 11
# prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# iters = 10000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# string = "k11-10000"

# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    
#     # Update long-term averages
#     eps_regret = eps_regret + (Gregrets - eps_regret) / (i + 1)
#     UCB_regret = UCB_regret + (Uregrets - UCB_regret) / (i + 1)
#     T_regret = T_regret + (Tregrets - T_regret) / (i + 1)        

    
# plt.figure(figsize=(8,6))
# plt.plot(eps_regret, label="Greedy")
# plt.plot(UCB_regret, label="UCB")
# plt.plot(T_regret, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Average Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

"""n= 300, iteration = 10000, k=11"""

# k = 11
# prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# iters = 100000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# string = "k11-100000"
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    
#     # Update long-term averages
#     eps_regret = eps_regret + (Gregrets - eps_regret) / (i + 1)
#     UCB_regret = UCB_regret + (Uregrets - UCB_regret) / (i + 1)
#     T_regret = T_regret + (Tregrets - T_regret) / (i + 1)        

    
# plt.figure(figsize=(8,6))
# plt.plot(eps_regret, label="Greedy")
# plt.plot(UCB_regret, label="UCB")
# plt.plot(T_regret, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Average Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

"""n= 300, iteration = 1000, k=5"""

k = 5
prob = [0.3, 0.5, 0.7, 0.83, 0.85]
iters = 1000
eps_regret = np.zeros(iters+1)
UCB_regret = np.zeros(iters+1)
T_regret = np.zeros(iters+1)
Runs = 300
string = "k51000"

# Run experiments
for i in range(Runs):
    # Initialize bandits
    Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
    Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
    Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    
    # Update long-term averages
    eps_regret = eps_regret + (Gregrets - eps_regret) / (i + 1)
    UCB_regret = UCB_regret + (Uregrets - UCB_regret) / (i + 1)
    T_regret = T_regret + (Tregrets - T_regret) / (i + 1)        

    
plt.figure(figsize=(8,6))
plt.plot(eps_regret, label="Greedy")
plt.plot(UCB_regret, label="UCB")
plt.plot(T_regret, label="$Thompson Sampling$")
plt.legend()
plt.xlabel("Time",fontsize=14)
plt.ylabel("Average Regret",fontsize=14)
plt.title("# of time-steps " + str(iters))
plt.savefig(string+".pdf")
plt.show()

# k = 5
# prob = [0.3, 0.5, 0.7, 0.83, 0.85]
# iters = 1000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# string = "k51000"
# # optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    
#     # Update long-term averages
#     eps_regret = eps_regret + (Gregrets - eps_regret) / (i + 1)
#     UCB_regret = UCB_regret + (Uregrets - UCB_regret) / (i + 1)
#     T_regret = T_regret + (Tregrets - T_regret) / (i + 1)        

    
# plt.figure(figsize=(8,6))
# plt.plot(eps_regret, label="Greedy")
# plt.plot(UCB_regret, label="UCB")
# plt.plot(T_regret, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Average Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

# k = 5
# prob = [0.3, 0.5, 0.7, 0.83, 0.85]
# iters = 100000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# string = "k5100000"
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    
#     # Update long-term averages
#     eps_regret = eps_regret + (Gregrets - eps_regret) / (i + 1)
#     UCB_regret = UCB_regret + (Uregrets - UCB_regret) / (i + 1)
#     T_regret = T_regret + (Tregrets - T_regret) / (i + 1)        

    
# plt.figure(figsize=(8,6))
# plt.plot(eps_regret, label="Greedy")
# plt.plot(UCB_regret, label="UCB")
# plt.plot(T_regret, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Average Regret",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

"""# Actions taken by each algorithm"""

k = 11
prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
iters = 1000
eps_regret = np.zeros(iters+1)
UCB_regret = np.zeros(iters+1)
T_regret = np.zeros(iters+1)
Runs = 300
Garr = []
Tarr = []
Uarr = []
string = "k11-1000action"
optimal_actions= np.zeros(iters+1)
optimal = np.argmax(prob)
# Run experiments
for i in range(Runs):
    # Initialize bandits
    Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
    Garr.append(Gactions)
    Gmean = calculate_optimal_action(iters,Garr,optimal)

    Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
    Uarr.append(Uactions)
    Umean = calculate_optimal_action(iters,Uarr,optimal)

    Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    Tarr.append(Tactions)
    Tmean = calculate_optimal_action(iters,Tarr,optimal)

    
plt.figure(figsize=(8,6))
plt.plot(Gmean, label="Greedy")
plt.plot(Umean, label="UCB")
plt.plot(Tmean, label="$Thompson Sampling$")
plt.legend()
plt.xlabel("Time",fontsize=14)
plt.ylabel("Mean Probability of choosing the best action",fontsize=14)
plt.title("# of time-steps " + str(iters))
plt.savefig(string+".pdf")
plt.show()

# k = 11
# prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# iters = 10000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k11-10000action"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gactions)
#     Gmean = calculate_optimal_action(iters,Garr,optimal)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Uactions)
#     Umean = calculate_optimal_action(iters,Uarr,optimal)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tactions)
#     Tmean = calculate_optimal_action(iters,Tarr,optimal)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Mean Probability of choosing the best action",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

# k = 11
# prob = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
# iters = 100000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k11-100000action"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gactions)
#     Gmean = calculate_optimal_action(iters,Garr,optimal)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Uactions)
#     Umean = calculate_optimal_action(iters,Uarr,optimal)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tactions)
#     Tmean = calculate_optimal_action(iters,Tarr,optimal)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Mean Probability of choosing the best action",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

"""K=5"""

k = 5
prob = [0.3, 0.5, 0.7, 0.83, 0.85]
iters = 1000
eps_regret = np.zeros(iters+1)
UCB_regret = np.zeros(iters+1)
T_regret = np.zeros(iters+1)
Runs = 300
Garr = []
Tarr = []
Uarr = []
string = "k51000action"
optimal_actions= np.zeros(iters+1)
optimal = np.argmax(prob)
# Run experiments
for i in range(Runs):
    # Initialize bandits
    Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
    Garr.append(Gactions)
    Gmean = calculate_optimal_action(iters,Garr,optimal)

    Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
    Uarr.append(Uactions)
    Umean = calculate_optimal_action(iters,Uarr,optimal)

    Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
    Tarr.append(Tactions)
    Tmean = calculate_optimal_action(iters,Tarr,optimal)

    
plt.figure(figsize=(8,6))
plt.plot(Gmean, label="Greedy")
plt.plot(Umean, label="UCB")
plt.plot(Tmean, label="$Thompson Sampling$")
plt.legend()
plt.xlabel("Time",fontsize=14)
plt.ylabel("Mean Probability of choosing the best action",fontsize=14)
plt.title("# of time-steps " + str(iters))
plt.savefig(string+".pdf")
plt.show()

# k = 5
# prob = [0.3, 0.5, 0.7, 0.83, 0.85]
# iters = 10000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k510000action"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gactions)
#     Gmean = calculate_optimal_action(iters,Garr,optimal)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Uactions)
#     Umean = calculate_optimal_action(iters,Uarr,optimal)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tactions)
#     Tmean = calculate_optimal_action(iters,Tarr,optimal)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Mean Probability of choosing the best action",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()

# k = 5
# prob = [0.3, 0.5, 0.7, 0.83, 0.85]
# iters = 100000
# eps_regret = np.zeros(iters+1)
# UCB_regret = np.zeros(iters+1)
# T_regret = np.zeros(iters+1)
# Runs = 300
# Garr = []
# Tarr = []
# Uarr = []
# string = "k5100000action"
# optimal_actions= np.zeros(iters+1)
# optimal = np.argmax(prob)
# # Run experiments
# for i in range(Runs):
#     # Initialize bandits
#     Gregrets, Gestimates,Gactions,Gstep_regret = Greedy(k, prob, iters)
#     Garr.append(Gactions)
#     Gmean = calculate_optimal_action(iters,Garr,optimal)

#     Uregrets, Uestimates,Uactions,Ustep_regret = UCB(k, prob, iters)
#     Uarr.append(Uactions)
#     Umean = calculate_optimal_action(iters,Uarr,optimal)

#     Tregrets, Testimates,Tactions,Tstep_regret = TSampling(k, prob, iters)
#     Tarr.append(Tactions)
#     Tmean = calculate_optimal_action(iters,Tarr,optimal)

    
# plt.figure(figsize=(8,6))
# plt.plot(Gmean, label="Greedy")
# plt.plot(Umean, label="UCB")
# plt.plot(Tmean, label="$Thompson Sampling$")
# plt.legend()
# plt.xlabel("Time",fontsize=14)
# plt.ylabel("Mean Probability of choosing the best action",fontsize=14)
# plt.title("# of time-steps " + str(iters))
# plt.savefig(string+".pdf")
# plt.show()