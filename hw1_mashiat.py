# -*- coding: utf-8 -*-
"""HW1_Mashiat

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JoS5b1_Jqz2zl5d3vQQuwrO6wI6SUixb
"""

### load packages
import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import string
import re
import matplotlib.pyplot as plt
import math
from matplotlib import rc
from google.colab import drive
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score,confusion_matrix,accuracy_score
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
import sklearn.metrics as metrics
import sklearn
from sklearn import linear_model
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn import svm

"""### Load Data"""

dfTrain = pd.read_csv('cleveland-train.csv') #Loading Training Dataset
dfTest = pd.read_csv('cleveland-test.csv') #Loading Testing Dataset
N = len(dfTrain)

"""### Preprocessing Data"""

dfTrain['heartdisease::category|0|1'] = dfTrain['heartdisease::category|0|1'].replace(0,-1) 
dfTrain.head()

dfTest['heartdisease::category|0|1'] = dfTest['heartdisease::category|0|1'].replace(0,-1) 
dfTest.head()

columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',
       'exang', 'oldpeak', 'slope', 'ca', 'thal']

# Augment bias in training data
X = dfTrain[columns].values
N,num_features = X.shape
bias = np.ones((N,1))
X = np.hstack((bias,X))
y = dfTrain['heartdisease::category|0|1'].values.reshape(-1,1)

# Augment bias in training data
X_test = dfTest[columns].values
N,num_features = X_test.shape
bias = np.ones((N,1))
X_test = np.hstack((bias,X_test))
y_test = dfTest['heartdisease::category|0|1'].values

"""### Logistic regression function"""

def trainning(X,y,eta): #eta learning rate
  w = np.full((1, len(X[0])), 0)
  epoch = 0
  while True:
    theta = (y*X)/(1+np.exp(y*(X.dot(w.T))))
    # print(theta)
    gt=-np.sum(theta,axis=0)/N
    # print(gt)

    #updating weights
    w= w-(eta*gt)
    if epoch % 100000000 == 0:
      print(epoch,gt.T)
    epoch += 1

    #early termination
    if (np.all(np.absolute(gt.T)<1/1000)):
      break       
  return w.T

#No early terminission
def trainning_modified(X,y,eta,n): #eta learning rate
  w = np.full((1, len(X[0])), 0)
  for epoch in range(0,n):
    theta = (y*X/(1+np.exp(y*X.dot(w.T))))
    gt=-np.sum(theta,axis=0)/N
    temp = 1
    w= w-(eta*gt)      
  return w.T

"""###Calculate Ein"""

def calculate_Ein(w,X,y):
  sum = 0
  for n in range(0,len(X)):
    sum = sum + np.log(1+np.exp(-(y[n]*(w.T.dot(X[n])))))[0]
  Ein=sum/N
  return Ein

def logistic_reg(X,y):
  w = trainning(X,y,0.00001)
  Ein = calculate_Ein(w,X,y)
  return w,Ein

#Noearly_terminission
def logistic_reg_modified(X,y,n):
  w = trainning_modified(X,y,0.00001,n)
  Ein = calculate_Ein(w,X,y)
  return w,Ein

weight,Ein = logistic_reg(X,y)

print("Estimated weight vector for the features",weight)

print("Training error Ein",Ein)

"""### Classsification Error"""

def find_test_error(X,y,w):
  actual_labels=y
  predicted_labels=[]
  for i in range(0,len(X)):
    if (X[i].dot(w)[0])>0:
      predicted_labels.append(1)
    else:
      predicted_labels.append(-1)
  diff = predicted_labels - actual_labels
  return (float(np.count_nonzero(diff)) / len(diff))

#Classification error on test data
print("Classification error on test data with early termination:",find_test_error(X_test,y_test,weight))

#Classification error on training data
print("Classification error on training data with early termination:find_test_error:",find_test_error(X,dfTrain['heartdisease::category|0|1'],weight))

"""### Performance Analysis by varying Maximum iteration"""

in_class_error=[]
classification_error_test =[]
classification_error_train =[]
epoch=[10000,100000,1000000]

# Epoch= 10,000
w1,e1 = logistic_reg_modified(X,y,10000)
c1_test = find_test_error(X_test,y_test,w1)
c1_train = find_test_error(X,dfTrain['heartdisease::category|0|1'],w1)
in_class_error.append(e1)
classification_error_test.append(c1_test)
classification_error_train.append(c1_train)

print("For 10,000 epoch in-sample error:%f, classification error on training data: %f and testing data: %f" % (e1,c1_train,c1_test))

# Epoch= 100,000
w2,e2 = logistic_reg_modified(X,y,100000)
c2_test = find_test_error(X_test,y_test,w2)
c2_train = find_test_error(X,dfTrain['heartdisease::category|0|1'],w2)
in_class_error.append(e2)
classification_error_test.append(c2_test)
classification_error_train.append(c2_train)
print("For 100,000 epoch in-sample error:%f, classification error on training data: %f and testing data: %f" % (e2,c2_train,c2_test))

# Epoch= 1000,000
w3,e3 = logistic_reg_modified(X,y,1000000)
c3_test = find_test_error(X_test,y_test,w3)
c3_train = find_test_error(X,dfTrain['heartdisease::category|0|1'],w3)
in_class_error.append(e3)
classification_error_test.append(c3_test)
classification_error_train.append(c3_train)
print("For 1,000,000 epoch in-class error:%f, classification error on training data: %f and testing data: %f" % (e3,c3_train,c3_test))

plt.plot(epoch, in_class_error, 'g', label='Ein')
plt.plot(epoch, classification_error_test, 'b', label='classification_error_test')
plt.plot(epoch, classification_error_train, 'r', label='classification_error_train')
plt.xticks([10000,100000,1000000])
plt.xlabel('Number of Epoch')
plt.ylabel('Error Rate')
plt.legend()

"""### Inbuilt logistic regression"""

start_time = time.time()
clf = LogisticRegression(solver='liblinear').fit(X, y)
print("Time taken by sklearn logistic regression for training: %s seconds\n" % (time.time() - start_time))

#Classification error on training data
prediction = clf.predict(X)
diff = prediction - dfTrain['heartdisease::category|0|1']
print("The classification error on traning data for inbuilt logistic regression:",(float(np.count_nonzero(diff)) / len(diff)))

#Classification error on test data
start_time1 = time.time()
prediction = clf.predict(X_test)
print("Time taken by sklearn logistic regression for prediction: %s seconds\n" % (time.time() - start_time1))
diff = prediction - y_test
print("The classification error on testing data for inbuilt logistic regression:",(float(np.count_nonzero(diff)) / len(diff)))

"""### Comparison with the 100000 epoch"""

# with termination condition at tolerance 0.001
start_time1 = time.time()
w2,e2 = logistic_reg(X,y)
print("Time taken by logistic_reg for training: %s seconds\n" % (time.time() - start_time))

start_time2 = time.time()
c2_test = find_test_error(X_test,y_test,w2)
print("Time taken by logistic_reg for prediction: %s seconds\n" % (time.time() - start_time2))

c2_train = find_test_error(X,dfTrain['heartdisease::category|0|1'],w2)
print("For 100,000 epoch in-sample error:%f, classification error on training data: %f and testing data: %f" % (e2,c2_train,c2_test))

"""### Scaling the dataset"""

def scaleColumns(df, cols_to_scale):
    for col in cols_to_scale:
        df[col] = (df[col] - df[col].mean())/df[col].std()
    return df

cols_to_scale = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',
       'exang', 'oldpeak', 'slope', 'ca', 'thal']

scaledTraining = scaleColumns(dfTrain,cols_to_scale)
scaledTesting = scaleColumns(dfTest,cols_to_scale)

X = scaledTraining[columns].values
N,num_features = X.shape
bias = np.ones((N,1))
X = np.hstack((bias,X))
y = scaledTraining['heartdisease::category|0|1'].values.reshape(-1,1)

X_test = scaledTesting[columns].values
N,num_features = X_test.shape
bias = np.ones((N,1))
X_test = np.hstack((bias,X_test))
y_test = scaledTesting['heartdisease::category|0|1'].values

"""###Trying Different learning rate"""

def trainning_scaled(X,y,eta): #eta learning rate
  w = np.full((1, len(X[0])), 0)
  epoch = 0
  while True:
    theta = (y*X)/(1+np.exp(y*(X.dot(w.T))))
    # print(eta)
    gt=-np.sum(theta,axis=0)/N
    # print(gt)

    #updating weights
    w= w-(eta*gt)
    # if epoch % 100000 == 0:
      # print(epoch,gt.T)
    epoch += 1

    #early termination
    if (np.all(np.absolute(gt.T)<1/100000)):
      break       
  return w.T,epoch

def logistic_reg_scaled(X,y,eta):
  w,epoch = trainning_scaled(X,y,eta)
  Ein = calculate_Ein(w,X,y)
  return w,Ein,epoch

"""Eta = 0.1"""

# Learning rate = 0.1
w,Ein,epoch = logistic_reg_scaled(X,y,0.1)
print("calculated weights",w)
print("in-class error on training set",Ein)
print("Number of iterration",epoch)
print("classification error on testing set",find_test_error(X_test,y_test,w))
print("classification error on training set",find_test_error(X,scaledTraining['heartdisease::category|0|1'].values,w))

# Learning rate = 0.01
w,Ein,epoch = logistic_reg_scaled(X,y,0.01)
print("calculated weights",w)
print("in-class error on training set",Ein)
print("Number of iterration",epoch)
print("classification error on testing set",find_test_error(X_test,y_test,w))
print("classification error on training set",find_test_error(X,scaledTraining['heartdisease::category|0|1'].values,w))

# Learning rate = 0.001
w,Ein,epoch = logistic_reg_scaled(X,y,0.001)
print("calculated weights",w)
print("in-class error on training set",Ein)
print("Number of iterration",epoch)
print("classification error on testing set",find_test_error(X_test,y_test,w))
print("classification error on training set",find_test_error(X,scaledTraining['heartdisease::category|0|1'].values,w))

# Learning rate = 0.0001
w,Ein,epoch = logistic_reg_scaled(X,y,0.0001)
print("calculated weights",w)
print("in-class error on training set",Ein)
print("Number of iterration",epoch)
print("classification error on testing set",find_test_error(X_test,y_test,w))
print("classification error on training set",find_test_error(X,scaledTraining['heartdisease::category|0|1'].values,w))

# Learning rate = 0.00001
w,Ein,epoch = logistic_reg_scaled(X,y,0.00001)
print("calculated weights",w)
print("in-class error on training set",Ein)
print("Number of iterration",epoch)
print("classification error on testing set",find_test_error(X_test,y_test,w))
print("classification error on training set",find_test_error(X,scaledTraining['heartdisease::category|0|1'].values,w))

# Learning rate = 0.000001
w,Ein,epoch = logistic_reg_scaled(X,y,0.000001)
print("calculated weights",w)
print("in-class error on training set",Ein)
print("Number of iterration",epoch)
print("classification error on testing set",find_test_error(X_test,y_test,w))
print("classification error on training set",find_test_error(X,scaledTraining['heartdisease::category|0|1'].values,w))

"""### **Question 2**

---

---

### Question 2: Comparing Different Classifiers
"""

### Preprocessing Dataset
# Augment bias in training data
X = dfTrain[columns].values
N,num_features = X.shape
bias = np.ones((N,1))
X = np.hstack((bias,X))
y = dfTrain['heartdisease::category|0|1'].values.reshape(-1,1)

# Augment bias in training data
X_test = dfTest[columns].values
N,num_features = X_test.shape
bias = np.ones((N,1))
X_test = np.hstack((bias,X_test))
y_test = dfTest['heartdisease::category|0|1'].values

"""### Random Forest

"""

#Random Forest
param_test = {
    'n_estimators': [20,50,100],
    'max_features': ['auto', 'sqrt', 'log2'],
    'class_weight': ['None','balanced','balanced_subsample']
}
gsearchRF = GridSearchCV(estimator =RandomForestClassifier(n_jobs=-1,max_features= 'auto',class_weight=None ,n_estimators=20, oob_score = True), 
 param_grid = param_test, scoring='accuracy',n_jobs=-1, cv=5)

train_modelRF = gsearchRF.fit(X, dfTrain['heartdisease::category|0|1'])
pred4 = train_modelRF.predict(X_test)
print("accuracy score for Random Forest : %.2f" % (accuracy_score(y_test, pred4) ))

print(gsearchRF.best_estimator_)

print("Cross Validation Error for random forest regression",1-gsearchRF.best_score_)

diff = pred4 - y_test
print("The classification error on test data for random forest regression:",(float(np.count_nonzero(diff)) / len(diff)))

#Classification error on training data
prediction =  train_modelRF.predict(X)
diff = prediction - dfTrain['heartdisease::category|0|1'].values
print("The classification error on traning data for random forest regression :",(float(np.count_nonzero(diff)) / len(diff)))

"""### SVM"""

# Support Vector Classifier
from sklearn import svm
sv = svm.SVC()
param_test = {
              'C': [.1, 1, 100, 1000],
              'gamma': ['scale'],
              'kernel': ['rbf', 'sigmoid','linear']
              }
gsearch = GridSearchCV(estimator =sv,param_grid = param_test, scoring='accuracy',cv=5)

train_modelSVC = gsearch.fit(X, dfTrain['heartdisease::category|0|1'])
pred4 = train_modelSVC.predict(X_test)
print("accuracy score for SVC: %.2f" % (accuracy_score(y_test, pred4) ))

print(gsearch.best_estimator_)

print("Cross Validation Error for Support Vector Classifier",1-gsearch.best_score_)

diff = pred4 - y_test
print("The classification error on testing data for SVC:",(float(np.count_nonzero(diff)) / len(diff)))

#Classification error on training data
prediction =  train_modelSVC.predict(X)
diff = prediction - dfTrain['heartdisease::category|0|1']
print("The classification error on traning data for SVC:",(float(np.count_nonzero(diff)) / len(diff)))

"""### Boosting Algorithm (XGboost)"""

#XGBoost
param_test = {
 'max_depth':[4,5,6],
 'min_child_weight':[4,5,6]
}
gsearchXG = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,
 min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,
 objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), 
 param_grid = param_test, scoring='accuracy',n_jobs=4, cv=5)
train_modelXG = gsearchXG.fit(X, dfTrain['heartdisease::category|0|1'])
ypred = train_modelXG.predict(X_test)
print("Accuracy score for XGBoost model: %.2f" % (accuracy_score(y_test, ypred) ))

print(gsearchXG.best_estimator_)

print("Cross Validation Error",1-gsearchXG.best_score_)